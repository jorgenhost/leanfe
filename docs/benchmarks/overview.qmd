---
title: "Performance Benchmarks"
subtitle: "YOCO + Sparse Matrices for Ultra-Fast Fixed Effects"
---

## Overview

leanfe is designed for speed and memory efficiency. It uses **YOCO (You Only Compress Once)** compression from [Wong et al. (2021)](https://arxiv.org/abs/2102.11297) combined with **sparse matrices** to dramatically reduce computation time and memory usage.

## How It Works

For **all standard error types** (IID, HC1, and clustered), leanfe automatically:

1. **Compresses data**: GROUP BY (regressors + fixed effects + cluster) â†’ compute `n`, `sum(y)`, `sum(yÂ²)` per group
2. **Builds sparse design matrix**: FE dummies as scipy.sparse/Matrix (13x faster than dense)
3. **Solves weighted least squares**: On compressed data with sparse operations (7x faster)
4. **Computes standard errors**: From grouped sufficient statistics
5. **For clustered SEs**: Uses sparse cluster matrix WÌƒ_C for vectorized score aggregation (Section 5.3.1)

This is **lossless** - coefficients and standard errors are identical to the full computation, but up to **23x faster** at scale.

See [Clustered SE Benchmarks](cluster-se.qmd) for detailed performance analysis of cluster-robust inference.

## Key Findings

| Metric | leanfe-Polars | leanfe-DuckDB |
|--------|---------------|---------------|
| **Speed** | âš¡ Fastest (0.08s @ 1M rows) | Fast (0.5s @ 1M rows) |
| **Memory** | Higher | ðŸ’¾ Minimal |
| **Compression** | 10-1000x for discrete regressors |
| **Speedup vs FWL** | 7-23x faster with YOCO+sparse |
| **Best for** | In-memory data | Large datasets |

### YOCO + Sparse vs Traditional FWL

| Dataset Size | YOCO+Sparse | FWL (cluster SEs) | Speedup |
|--------------|-------------|-------------------|---------|
| 100K rows | 0.05s | 0.07s | 1.3x |
| 1M rows | 0.08s | 0.59s | **7.3x** |
| 5M rows | 0.15s | 3.42s | **22.8x** |

## Benchmark Results

```{python}
#| echo: false
#| output: true

import numpy as np
import polars as pl
import time
from leanfe import leanfe

# Generate benchmark data
np.random.seed(42)

def run_benchmark(n_obs, n_fe1=500, n_fe2=100):
    """Run benchmark for given size."""
    fe1 = np.random.randint(1, n_fe1 + 1, n_obs)
    fe2 = np.random.randint(1, n_fe2 + 1, n_obs)
    fe1_effects = np.random.normal(0, 1, n_fe1 + 1)[fe1]
    fe2_effects = np.random.normal(0, 0.5, n_fe2 + 1)[fe2]
    
    treatment = np.random.binomial(1, 0.3, n_obs).astype(float)
    x1 = np.random.normal(0, 1, n_obs)
    y = 2.5 * treatment + 1.5 * x1 + fe1_effects + fe2_effects + np.random.normal(0, 1, n_obs)
    
    df = pl.DataFrame({
        "y": y, "treatment": treatment, "x1": x1,
        "fe1": fe1, "fe2": fe2,
    })
    
    results = {}
    
    # Polars
    start = time.time()
    leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="polars")
    results["polars"] = time.time() - start
    
    # DuckDB
    start = time.time()
    leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="duckdb")
    results["duckdb"] = time.time() - start
    
    return results

# Run benchmarks
sizes = [100_000, 500_000, 1_000_000]
benchmark_results = []

for n in sizes:
    result = run_benchmark(n)
    benchmark_results.append({
        "n_obs": n,
        "polars_time": result["polars"],
        "duckdb_time": result["duckdb"],
    })

# Display results
print("Benchmark Results (Two-way FE, IID SE)")
print("=" * 50)
print(f"{'Observations':<15} {'Polars':<12} {'DuckDB':<12}")
print("-" * 50)
for r in benchmark_results:
    print(f"{r['n_obs']:>12,}   {r['polars_time']:>8.2f}s    {r['duckdb_time']:>8.2f}s")
```

## Performance Chart

```{python}
#| echo: false
#| output: true
#| fig-cap: "Execution Time by Dataset Size"

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 6))

n_obs = [r["n_obs"] / 1_000_000 for r in benchmark_results]
polars_times = [r["polars_time"] for r in benchmark_results]
duckdb_times = [r["duckdb_time"] for r in benchmark_results]

x = range(len(n_obs))
width = 0.35

bars1 = ax.bar([i - width/2 for i in x], polars_times, width, label='Polars', color='steelblue')
bars2 = ax.bar([i + width/2 for i in x], duckdb_times, width, label='DuckDB', color='coral')

ax.set_xlabel('Dataset Size (millions of observations)', fontsize=12)
ax.set_ylabel('Time (seconds)', fontsize=12)
ax.set_title('leanfe Performance: Polars vs DuckDB', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels([f'{n:.1f}M' for n in n_obs])
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# Add value labels
for bar in bars1:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}s', xy=(bar.get_x() + bar.get_width()/2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=9)
for bar in bars2:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}s', xy=(bar.get_x() + bar.get_width()/2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()
```

## Memory Efficiency

The DuckDB backend's key advantage is memory efficiency:

| Dataset Size | Polars Memory | DuckDB Memory |
|--------------|---------------|---------------|
| 1M obs | ~150 MB | ~50 MB |
| 5M obs | ~600 MB | ~100 MB |
| 10M obs | ~1.2 GB | ~150 MB |

**DuckDB can process datasets larger than available RAM** by streaming from parquet files.

## When to Use Each Backend

### Use Polars (default) when:
- Speed is the priority
- Data fits comfortably in memory
- Running many regressions on the same data

### Use DuckDB when:
- Dataset is larger than available RAM
- Memory is constrained
- Reading directly from parquet files

## Compression Ratio

The compression ratio depends on the number of unique (regressor, FE) combinations:

```{python}
#| echo: true
#| output: true

# Show compression in action
np.random.seed(42)
n = 1_000_000

# Discrete regressors = high compression
df_discrete = pl.DataFrame({
    "y": np.random.normal(0, 1, n),
    "treatment": np.random.binomial(1, 0.3, n).astype(float),  # Binary
    "fe1": np.random.randint(1, 501, n),
    "fe2": np.random.randint(1, 101, n),
})

result = leanfe(data=df_discrete, formula="y ~ treatment | fe1 + fe2", vcov="iid")
print(f"Discrete regressors (1M obs):")
print(f"  Original rows:   {result['n_obs']:,}")
print(f"  Compressed rows: {result['n_compressed']:,}")
print(f"  Compression:     {result['compression_ratio']:.1%}")
```

## Coefficient Validation

All benchmarks verify that both backends produce identical coefficients:

```{python}
#| echo: true
#| output: true

# Verify coefficients match
np.random.seed(42)
n = 100_000
df = pl.DataFrame({
    "y": np.random.normal(0, 1, n),
    "treatment": np.random.binomial(1, 0.3, n).astype(float),
    "x1": np.random.normal(0, 1, n),
    "fe1": np.random.randint(1, 101, n),
    "fe2": np.random.randint(1, 51, n),
})

result_polars = leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="polars")
result_duckdb = leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="duckdb")

print("Coefficient Comparison:")
print(f"  Polars treatment: {result_polars['coefficients']['treatment']:.6f}")
print(f"  DuckDB treatment: {result_duckdb['coefficients']['treatment']:.6f}")
print(f"  Difference: {abs(result_polars['coefficients']['treatment'] - result_duckdb['coefficients']['treatment']):.2e}")
```

## Methodology

See [Benchmark Methodology](methodology.qmd) for details on:

- Data generation process
- Hardware and software specifications
- Validation approach
- Reproducible benchmark scripts
