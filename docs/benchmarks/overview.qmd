---
title: "Performance Benchmarks"
subtitle: "Comparing leanfe vs pyfixest and fixest"
---

## Overview

leanfe is designed for speed and memory efficiency. It automatically selects the optimal algorithm based on your data:

- **YOCO compression + sparse matrices** for low/medium-cardinality FEs (< 10K levels)
- **FWL iterative demeaning** for high-cardinality FEs (> 10K levels)

Both approaches produce **identical results** to traditional methods, just faster.

## How It Works

### Smart Strategy Selection

leanfe analyzes your fixed effects and automatically chooses the fastest approach:

| FE Cardinality | Strategy | Why |
|----------------|----------|-----|
| Low (< 10K levels) | YOCO + Sparse | Compression reduces data dramatically; sparse matrices are efficient |
| High (> 10K levels) | FWL Demeaning | Avoids huge sparse matrices; iterative demeaning scales better |

You don't need to configure anything â€” leanfe picks the optimal strategy automatically.

### YOCO Compression (Low-Cardinality FEs)

From [Wong et al. (2021)](https://arxiv.org/abs/2102.11297):

1. **Compress data**: GROUP BY (regressors + FEs) â†’ compute sufficient statistics per group
2. **Build sparse design matrix**: FE dummies as scipy.sparse/Matrix
3. **Solve weighted least squares**: On compressed data with sparse operations

### FWL Demeaning (High-Cardinality FEs)

For FEs with many levels (e.g., customer IDs):

1. **Iteratively demean** Y and X within each FE group
2. **Sort FEs by cardinality** (low-card first) for faster convergence
3. **Run OLS** on fully demeaned data

### Dual Backend Architecture

| Backend | Strengths | Best For |
|---------|-----------|----------|
| **Polars** | Blazing fast, lazy evaluation, zero-copy | Speed when data fits in memory |
| **DuckDB** | Streams from disk, constant memory | Datasets larger than RAM |

Both backends use the same smart strategy selection. The choice is about **speed vs memory**, not features.

See [Clustered SE Benchmarks](cluster-se.qmd) for detailed performance analysis of cluster-robust inference.

## Key Findings

| Package | Speed | Memory | Best For |
|---------|-------|--------|----------|
| **leanfe-Polars** | âš¡ Fastest | Moderate | Maximum speed, in-memory data |
| **leanfe-DuckDB** | Fast | ðŸ’¾ Minimal | Large datasets, memory-constrained |
| **pyfixest** | Slow | High | Compatibility with fixest syntax |
| **fixest** | Fast | High | R users who need maximum compatibility |

**Key takeaways:**

- **Python**: leanfe-Polars is **10-15x faster** than pyfixest with **20x less memory**
- **R**: leanfe competes with fixest on speed while using less memory
- **DuckDB backend**: Enables processing datasets **larger than RAM**

## Benchmark Results

All benchmarks use standardized data generation (see [Methodology](methodology.qmd)):

- **FE1**: 500 levels, **FE2**: 100 levels
- **Regressors**: Binary treatment + discrete x1 (0, 1, 2)
- **True coefficients**: treatment=2.5, x1=1.5

### Python Benchmarks

```{python}
#| echo: false
#| output: true

import numpy as np
import polars as pl
import time
import tracemalloc
import gc
from leanfe import leanfe
import pyfixest as pf

np.random.seed(42)

def generate_benchmark_data(n_obs, n_fe1=500, n_fe2=100):
    """Standardized benchmark data generation."""
    fe1 = np.random.randint(1, n_fe1 + 1, n_obs)
    fe2 = np.random.randint(1, n_fe2 + 1, n_obs)
    fe1_effects = np.random.normal(0, 1, n_fe1 + 1)[fe1]
    fe2_effects = np.random.normal(0, 0.5, n_fe2 + 1)[fe2]
    
    treatment = np.random.binomial(1, 0.3, n_obs).astype(float)
    x1 = np.random.choice([0.0, 1.0, 2.0], n_obs)  # Discrete for compression
    y = 2.5 * treatment + 1.5 * x1 + fe1_effects + fe2_effects + np.random.normal(0, 1, n_obs)
    
    return pl.DataFrame({
        "y": y, "treatment": treatment, "x1": x1,
        "fe1": fe1, "fe2": fe2,
    })

def run_benchmark(n_obs, include_pyfixest=True):
    """Run benchmark with time and memory measurement."""
    gc.collect()
    df = generate_benchmark_data(n_obs)
    results = {"n_obs": n_obs}
    
    # leanfe Polars benchmark
    gc.collect()
    tracemalloc.start()
    start = time.time()
    result_polars = leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="polars")
    results["py_polars_time"] = time.time() - start
    _, results["py_polars_mem"] = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    results["compression"] = result_polars.get("compression_ratio", 1.0)
    
    # leanfe DuckDB benchmark
    gc.collect()
    tracemalloc.start()
    start = time.time()
    leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="duckdb")
    results["py_duckdb_time"] = time.time() - start
    _, results["py_duckdb_mem"] = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    # pyfixest benchmark (skip for very large datasets - too slow)
    if include_pyfixest and n_obs <= 1_000_000:
        gc.collect()
        df_pd = df.to_pandas()
        tracemalloc.start()
        start = time.time()
        pf.feols("y ~ treatment + x1 | fe1 + fe2", data=df_pd, vcov="iid")
        results["pyfixest_time"] = time.time() - start
        _, results["pyfixest_mem"] = tracemalloc.get_traced_memory()
        tracemalloc.stop()
    else:
        results["pyfixest_time"] = None
        results["pyfixest_mem"] = None
    
    return results

# Run benchmarks: 100K, 1M, 10M
sizes = [100_000, 1_000_000, 10_000_000]
py_results = []

print("Python IID Standard Errors Benchmark (Two-way FE)")
print("=" * 95)
print(f"{'Obs':>12} {'leanfe-Polars':>14} {'leanfe-DuckDB':>14} {'pyfixest':>12} {'Speedup':>10}")
print("-" * 95)

for n in sizes:
    result = run_benchmark(n, include_pyfixest=(n <= 1_000_000))
    py_results.append(result)
    pyfixest_str = f"{result['pyfixest_time']:.2f}s" if result['pyfixest_time'] else "skipped"
    speedup = f"{result['pyfixest_time'] / result['py_polars_time']:.1f}x" if result['pyfixest_time'] else "-"
    print(f"{n:>12,} {result['py_polars_time']:>13.2f}s {result['py_duckdb_time']:>13.2f}s {pyfixest_str:>12} {speedup:>10}")

print("\nNote: pyfixest skipped for 10M rows (too slow)")
```

### R Benchmarks

::: {.callout-note}
R benchmarks compare leanfe (Polars and DuckDB backends) against fixest, the gold standard for fixed effects in R.
:::

```{r}
#| echo: false
#| output: true
#| eval: true

# Source common setup (configures reticulate for CI)
source("../_common.R")

library(polars)
library(duckdb)
library(DBI)
library(Matrix)
library(fixest)

set.seed(42)

generate_benchmark_data <- function(n_obs, n_fe1 = 500, n_fe2 = 100) {
  fe1 <- sample(1:n_fe1, n_obs, replace = TRUE)
  fe2 <- sample(1:n_fe2, n_obs, replace = TRUE)
  fe1_effects <- rnorm(n_fe1 + 1)[fe1]
  fe2_effects <- rnorm(n_fe2 + 1, sd = 0.5)[fe2]
  
  treatment <- rbinom(n_obs, 1, 0.3)
  x1 <- sample(c(0, 1, 2), n_obs, replace = TRUE)
  y <- 2.5 * treatment + 1.5 * x1 + fe1_effects + fe2_effects + rnorm(n_obs)
  
  pl$DataFrame(
    y = y, treatment = as.numeric(treatment), x1 = as.numeric(x1),
    fe1 = fe1, fe2 = fe2
  )
}

run_r_benchmark <- function(n_obs) {
  df <- generate_benchmark_data(n_obs)
  
  # leanfe Polars benchmark
  start <- Sys.time()
  result_polars <- leanfe_polars(data = df, formula = "y ~ treatment + x1 | fe1 + fe2", vcov = "iid")
  r_polars_time <- as.numeric(difftime(Sys.time(), start, units = "secs"))
  
  # leanfe DuckDB benchmark
  start <- Sys.time()
  result_duckdb <- leanfe_duckdb(data = df, formula = "y ~ treatment + x1 | fe1 + fe2", vcov = "iid")
  r_duckdb_time <- as.numeric(difftime(Sys.time(), start, units = "secs"))
  
  # fixest benchmark
  df_r <- as.data.frame(df)
  start <- Sys.time()
  feols(y ~ treatment + x1 | fe1 + fe2, data = df_r, vcov = "iid")
  fixest_time <- as.numeric(difftime(Sys.time(), start, units = "secs"))
  
  list(
    n_obs = n_obs,
    r_polars_time = r_polars_time,
    r_duckdb_time = r_duckdb_time,
    fixest_time = fixest_time,
    compression = result_polars$compression_ratio
  )
}

# Run R benchmarks: 100K, 1M, 10M
sizes <- c(100000, 1000000, 10000000)

cat("R IID Standard Errors Benchmark (Two-way FE)\n")
cat(strrep("=", 95), "\n")
cat(sprintf("%12s %14s %14s %12s %10s\n", "Obs", "leanfe-Polars", "leanfe-DuckDB", "fixest", "Speedup"))
cat(strrep("-", 95), "\n")

for (n in sizes) {
  result <- run_r_benchmark(n)
  speedup <- result$fixest_time / result$r_polars_time
  cat(sprintf("%12s %13.2fs %13.2fs %11.2fs %9.1fx\n", 
              format(n, big.mark = ",", scientific = FALSE), 
              result$r_polars_time, 
              result$r_duckdb_time,
              result$fixest_time,
              speedup))
}
```

## Performance Charts (Python)

```{python}
#| echo: false
#| output: true
#| fig-cap: "Python Performance Comparison: leanfe vs pyfixest"

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

n_obs = [r["n_obs"] / 1_000_000 for r in py_results]
polars_times = [r["py_polars_time"] for r in py_results]
duckdb_times = [r["py_duckdb_time"] for r in py_results]
pyfixest_times = [r["pyfixest_time"] if r["pyfixest_time"] else None for r in py_results]

polars_mem = [r["py_polars_mem"] / 1024 / 1024 for r in py_results]
duckdb_mem = [r["py_duckdb_mem"] / 1024 / 1024 for r in py_results]
pyfixest_mem = [r["pyfixest_mem"] / 1024 / 1024 if r["pyfixest_mem"] else None for r in py_results]

# Left plot: Execution time
ax1.plot(n_obs, polars_times, 'o-', label='leanfe-Polars', color='steelblue', linewidth=2, markersize=8)
ax1.plot(n_obs, duckdb_times, 's-', label='leanfe-DuckDB', color='coral', linewidth=2, markersize=8)
# Plot pyfixest only for available data points
pf_n = [n for n, t in zip(n_obs, pyfixest_times) if t is not None]
pf_t = [t for t in pyfixest_times if t is not None]
if pf_n:
    ax1.plot(pf_n, pf_t, '^-', label='pyfixest', color='gray', linewidth=2, markersize=8)
ax1.set_xlabel('Dataset Size (millions)', fontsize=12)
ax1.set_ylabel('Time (seconds)', fontsize=12)
ax1.set_title('Execution Time', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right plot: Memory usage
ax2.plot(n_obs, polars_mem, 'o-', label='leanfe-Polars', color='steelblue', linewidth=2, markersize=8)
ax2.plot(n_obs, duckdb_mem, 's-', label='leanfe-DuckDB', color='coral', linewidth=2, markersize=8)
# Plot pyfixest memory only for available data points
pf_m = [m for m in pyfixest_mem if m is not None]
if pf_n and pf_m:
    ax2.plot(pf_n, pf_m, '^-', label='pyfixest', color='gray', linewidth=2, markersize=8)
ax2.set_xlabel('Dataset Size (millions)', fontsize=12)
ax2.set_ylabel('Peak Memory (MB)', fontsize=12)
ax2.set_title('Memory Usage', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Memory Efficiency

**DuckDB can process datasets larger than available RAM** by streaming from parquet files. The memory measurements above show peak Python heap usage during regression.

## When to Use Each Backend

### Use Polars (default) when:
- Speed is the priority
- Data fits comfortably in memory
- Running many regressions on the same data

### Use DuckDB when:
- Dataset is larger than available RAM
- Memory is constrained
- Reading directly from parquet files

## Compression Ratio

The compression ratio depends on the number of unique (regressor, FE) combinations. With discrete regressors (binary treatment + categorical x1), YOCO achieves significant compression:

```{python}
#| echo: true
#| output: true

# Show compression across different dataset sizes
print("Compression Ratio by Dataset Size")
print("=" * 50)
print(f"{'Observations':>15} {'Compressed':>15} {'Ratio':>10}")
print("-" * 50)

for r in py_results:
    n_compressed = int(r["n_obs"] * r["compression"])
    print(f"{r['n_obs']:>15,} {n_compressed:>15,} {r['compression']:>9.1%}")
```

## Coefficient Validation

All benchmarks verify that both backends produce identical coefficients:

```{python}
#| echo: true
#| output: true

# Verify coefficients match
np.random.seed(42)
n = 100_000
df = pl.DataFrame({
    "y": np.random.normal(0, 1, n),
    "treatment": np.random.binomial(1, 0.3, n).astype(float),
    "x1": np.random.normal(0, 1, n),
    "fe1": np.random.randint(1, 101, n),
    "fe2": np.random.randint(1, 51, n),
})

result_polars = leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="polars")
result_duckdb = leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid", backend="duckdb")

print("Coefficient Comparison:")
print(f"  Polars treatment: {result_polars['coefficients']['treatment']:.6f}")
print(f"  DuckDB treatment: {result_duckdb['coefficients']['treatment']:.6f}")
print(f"  Difference: {abs(result_polars['coefficients']['treatment'] - result_duckdb['coefficients']['treatment']):.2e}")
```

## Methodology

See [Benchmark Methodology](methodology.qmd) for details on:

- Data generation process
- Hardware and software specifications
- Validation approach
- Reproducible benchmark scripts
